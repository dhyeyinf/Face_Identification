import os
import numpy as np
import matplotlib.pyplot as plt

DATA_DIR = '/datasets/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'

# Get all the person labels
labels = []
for person_dir in os.listdir(DATA_DIR):
    if os.path.isdir(os.path.join(DATA_DIR, person_dir)):
        num_images = len(os.listdir(os.path.join(DATA_DIR, person_dir)))
        if num_images > 5:
            labels.append(person_dir)

# Count the occurrences of each label
label_counts = {label: 0 for label in labels}
for label in labels:
    label_counts[label] = len(os.listdir(os.path.join(DATA_DIR, label)))

# Plot the histogram
plt.figure(figsize=(10, 6))
plt.bar(range(len(label_counts)), label_counts.values(), align='center', alpha=0.7)
plt.xticks(range(len(label_counts)), label_counts.keys(), rotation=90)
plt.xlabel('Person Name')
plt.ylabel('Number of Photos')
plt.title('Number of Photos per Person')
plt.tight_layout()
plt.show()

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

DATA_DIR = '/datasets/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'
IMAGE_SIZE = (50, 50)

data = []
labels = []

# Iterate through each person's directory in the dataset
for person_dir in os.listdir(DATA_DIR):
    if os.path.isdir(os.path.join(DATA_DIR, person_dir)):
        # Count the number of images for the current person
        num_images = len(os.listdir(os.path.join(DATA_DIR, person_dir)))

        # Only include classes with more than 70 images
        if num_images > 70:
            for image_file in os.listdir(os.path.join(DATA_DIR, person_dir)):
                image_path = os.path.join(DATA_DIR, person_dir, image_file)
                image = cv2.imread(image_path)
                if image is not None:
                    image = cv2.resize(image, IMAGE_SIZE)
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    # image = image / 255.0  # Normalize pixel values
                    data.append(image)
                    labels.append(person_dir)

# Convert lists to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

print("Number of samples:", len(data))
print("Number of unique labels:", len(np.unique(labels)))

print("Shape of filtered training data:", X_train.shape)
print("Shape of filtered training labels:", y_train.shape)
print("Shape of filtered test data:", X_test.shape)
print("Shape of filtered test labels:", y_test.shape)

unique_labels_train, label_counts_train = np.unique(y_train, return_counts=True)
filtered_labels_train = unique_labels_train[label_counts_train > 0]

filtered_indices_train = np.isin(y_train, filtered_labels_train)
X_train_filtered = X_train[filtered_indices_train]
y_train_filtered = y_train[filtered_indices_train]

print("Shape of filtered training data:", X_train_filtered.shape)
print("Shape of filtered training labels:", y_train_filtered.shape)

unique_labels_test, label_counts_test = np.unique(y_test, return_counts=True)
filtered_labels_test = unique_labels_test[label_counts_test > 0]

filtered_indices_test = np.isin(y_test, filtered_labels_test)
X_test_filtered = X_test[filtered_indices_test]
y_test_filtered = y_test[filtered_indices_test]

print("Shape of filtered test data:", X_test_filtered.shape)
print("Shape of filtered test labels:", y_test_filtered.shape)

import numpy as np

unique_labels_train_filtered = np.unique(y_train_filtered)
print("Unique labels in filtered training data:", unique_labels_train_filtered)

unique_labels_test_filtered = np.unique(y_test_filtered)
print("Unique labels in filtered test data:", unique_labels_test_filtered)

import csv
import numpy as np

# Assuming y_train_filtered is your array of labels
y_train_filtered = np.array(['Ariel_Sharon', 'Colin_Powell', 'Donald_Rumsfeld', 'George_W_Bush',
 'Gerhard_Schroeder', 'Hugo_Chavez', 'Tony_Blair'])

unique_labels_train_filtered = np.unique(y_train_filtered)

# Specify the file path
csv_file_path = "unique_labels.csv"

# Write the unique labels to a CSV file
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Unique labels"])
    for label in unique_labels_train_filtered:
        writer.writerow([label])

print("Unique labels saved to", csv_file_path)

unique_labels_train, label_counts_train = np.unique(y_train, return_counts=True)
filtered_labels_train = unique_labels_train[label_counts_train > 0]

filtered_indices_train = np.isin(y_train, filtered_labels_train)
X_train_filtered = X_train[filtered_indices_train]
y_train_filtered = y_train[filtered_indices_train]

print("Shape of filtered training data:", X_train_filtered.shape)
print("Shape of filtered training labels:", y_train_filtered.shape)

unique_labels_test, label_counts_test = np.unique(y_test, return_counts=True)
filtered_labels_test = unique_labels_test[label_counts_test > 0]

filtered_indices_test = np.isin(y_test, filtered_labels_test)
X_test_filtered = X_test[filtered_indices_test]
y_test_filtered = y_test[filtered_indices_test]

print("Shape of filtered test data:", X_test_filtered.shape)
print("Shape of filtered test labels:", y_test_filtered.shape)

import numpy as np

unique_labels_train_filtered = np.unique(y_train_filtered)
print("Unique labels in filtered training data:", unique_labels_train_filtered)

unique_labels_test_filtered = np.unique(y_test_filtered)
print("Unique labels in filtered test data:", unique_labels_test_filtered)

import csv
import numpy as np

# Assuming y_train_filtered is your array of labels
y_train_filtered = np.array(['Ariel_Sharon', 'Colin_Powell', 'Donald_Rumsfeld', 'George_W_Bush',
 'Gerhard_Schroeder', 'Hugo_Chavez', 'Tony_Blair'])

unique_labels_train_filtered = np.unique(y_train_filtered)

# Specify the file path
csv_file_path = "unique_labels.csv"

# Write the unique labels to a CSV file
with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Unique labels"])
    for label in unique_labels_train_filtered:
        writer.writerow([label])

print("Unique labels saved to", csv_file_path)

import numpy as np

class LDA:
    def fit(self, X, y):
        class_labels = np.unique(y)
        class_means = []
        overall_mean = np.mean(X, axis=0)

        for label in class_labels:
            class_X = X[y == label]
            class_means.append(np.mean(class_X, axis=0))

        self.class_means = np.array(class_means)
        self.overall_mean = overall_mean

        #between-class scatter matrix
        between_class_scatter = np.zeros((X.shape[1], X.shape[1]))
        for label, class_mean in zip(class_labels, class_means):
            n_samples = X[y == label].shape[0]
            diff = (class_mean - overall_mean).reshape(-1, 1)
            between_class_scatter += n_samples * np.dot(diff, diff.T)

        #within-class scatter matrix
        within_class_scatter = np.zeros((X.shape[1], X.shape[1]))
        for label, class_mean in zip(class_labels, class_means):
            class_X = X[y == label]
            diff = class_X - class_mean
            within_class_scatter += np.dot(diff.T, diff)

        eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter) @ between_class_scatter)

        sorted_indices = np.argsort(eigen_values)[::-1]
        self.eigen_vectors = eigen_vectors[:, sorted_indices]

    def transform(self, X, n_components):
        return np.dot(X, self.eigen_vectors[:, :n_components])

from PIL import Image

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

X_train_2d = X_train_filtered.reshape(X_train_filtered.shape[0], -1)
X_test_2d = X_test_filtered.reshape(X_test_filtered.shape[0], -1)

lda = LDA(n_components=None)

X_train_lda = lda.fit_transform(X_train_2d, y_train_filtered)
X_test_lda = lda.transform(X_test_2d)
print(X_test_2d.shape)
joblib.dump(lda, 'lda_model.pkl')

classifiers = {
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

for clf_name, clf in classifiers.items():
  clf.fit(X_train_lda, y_train_filtered)
  test_image = "/content/tonyy.jpg"
  print("clf = ", clf_name)
  # Load and preprocess the test image
  image = Image.open(test_image)
  resized_image = image.resize((50, 50))
  image_array = np.array(resized_image)
  flat_image_array = image_array.flatten()
  test_features = flat_image_array.reshape(1, -1)
  test_lda = lda.transform(test_features)
  test_pred = clf.predict(test_lda).reshape(-1,1)
  decoded_pred = encoder.inverse_transform(test_pred[0])
  print("Prediction: ", decoded_pred)
  y_pred = clf.predict(X_test_lda)
  accuracy = accuracy_score(y_test_filtered, y_pred)
  print(f"{clf_name} accuracy:", accuracy)


from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y_train_filtered = encoder.fit_transform(y_train_filtered)
y_test_filtered = encoder.fit_transform(y_test_filtered)
# Now, y_train_encoded contains the encoded version of y_train_filtered


from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import joblib

# Train the SVM model
svm_model = SVC()
svm_model.fit(X_train_lda, y_train_filtered)
joblib.dump(svm_model, 'svm_model.pkl')
# # Save the trained model
# joblib.dump(svm_model, 'svm_model.joblib')

# # Load the saved model
# loaded_model = joblib.load('svm_model.joblib')

# Load and preprocess the test image
test_image = "/content/tonyy.jpg"
image = Image.open(test_image)
resized_image = image.resize((50, 50))
image_array = np.array(resized_image)
flat_image_array = image_array.flatten()
test_features = flat_image_array.reshape(1, -1)
test_lda = lda.transform(test_features)

# Predict using the loaded model
test_pred = svm_model.predict(test_lda).reshape(-1, 1)
decoded_pred = encoder.inverse_transform(test_pred[0])
print("Prediction:", decoded_pred)

# Predict on test set and calculate accuracy
y_pred = svm_model.predict(X_test_lda)
accuracy = accuracy_score(y_test_filtered, y_pred)
print("Model accuracy:", accuracy)


from skimage.feature import local_binary_pattern
from skimage.feature import hog
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from skimage.color import rgb2gray

def extract_lbp_features(images):
    lbp_features = []
    for img in images:
        gray_img = rgb2gray(img)
        lbp = local_binary_pattern(gray_img, P=8, R=1, method='uniform')
        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 10))
        lbp_features.append(hist)
    return np.array(lbp_features)

lbp_train_features = extract_lbp_features(X_train_filtered)
lbp_test_features = extract_lbp_features(X_test_filtered)

import numpy as np
np.save('lbp_train_features.npy', lbp_train_features)
np.save('lbp_test_features.npy', lbp_test_features)

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier

# Support Vector Machine (SVM) Classifier
svm_classifier = SVC()
svm_classifier.fit(lbp_train_features, y_train_filtered)
svm_train_pred = svm_classifier.predict(lbp_train_features)
svm_test_pred = svm_classifier.predict(lbp_test_features)
svm_train_accuracy = accuracy_score(y_train_filtered, svm_train_pred)
svm_test_accuracy = accuracy_score(y_test_filtered, svm_test_pred)
print("SVM Classifier Training Accuracy:", svm_train_accuracy)
print("SVM Classifier Testing Accuracy:", svm_test_accuracy)

# Random Forest Classifier
rf_classifier = RandomForestClassifier()
rf_classifier.fit(lbp_train_features, y_train_filtered)
rf_train_pred = rf_classifier.predict(lbp_train_features)
rf_test_pred = rf_classifier.predict(lbp_test_features)
rf_train_accuracy = accuracy_score(y_train_filtered, rf_train_pred)
rf_test_accuracy = accuracy_score(y_test_filtered, rf_test_pred)
print("Random Forest Classifier Training Accuracy:", rf_train_accuracy)
print("Random Forest Classifier Testing Accuracy:", rf_test_accuracy)

# K-Nearest Neighbors (KNN) Classifier
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(lbp_train_features, y_train_filtered)
knn_train_pred = knn_classifier.predict(lbp_train_features)
knn_test_pred = knn_classifier.predict(lbp_test_features)
knn_train_accuracy = accuracy_score(y_train_filtered, knn_train_pred)
knn_test_accuracy = accuracy_score(y_test_filtered, knn_test_pred)
print("KNN Classifier Training Accuracy:", knn_train_accuracy)
print("KNN Classifier Testing Accuracy:", knn_test_accuracy)

# Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()
gb_classifier.fit(lbp_train_features, y_train_filtered)
gb_train_pred = gb_classifier.predict(lbp_train_features)
gb_test_pred = gb_classifier.predict(lbp_test_features)
gb_train_accuracy = accuracy_score(y_train_filtered, gb_train_pred)
gb_test_accuracy = accuracy_score(y_test_filtered, gb_test_pred)
print("Gradient Boosting Classifier Training Accuracy:", gb_train_accuracy)
print("Gradient Boosting Classifier Testing Accuracy:", gb_test_accuracy)

from sklearn.linear_model import LogisticRegression

# Logistic Regression Classifier
lr_classifier = LogisticRegression(max_iter=500)
lr_classifier.fit(lbp_train_features, y_train_filtered)
lr_train_pred = lr_classifier.predict(lbp_train_features)
lr_test_pred = lr_classifier.predict(lbp_test_features)
lr_train_accuracy = accuracy_score(y_train_filtered, lr_train_pred)
lr_test_accuracy = accuracy_score(y_test_filtered, lr_test_pred)
print("Logistic Regression Classifier Training Accuracy:", lr_train_accuracy)
print("Logistic Regression Classifier Testing Accuracy:", lr_test_accuracy)


from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

lbp_train_features = lbp_train_features.reshape(X_train_filtered.shape[0], -1)
lbp_test_features = lbp_test_features.reshape(X_test_filtered.shape[0], -1)

lda = LDA(n_components=None)
X_train_lda = lda.fit_transform(lbp_train_features, y_train_filtered)
X_test_lda = lda.transform(lbp_test_features)

classifiers = {
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

for clf_name, clf in classifiers.items():
    clf.fit(X_train_lda, y_train_filtered)
    y_pred = clf.predict(X_test_lda)
    accuracy = accuracy_score(y_test_filtered, y_pred)
    print(f"{clf_name} accuracy:", accuracy)

